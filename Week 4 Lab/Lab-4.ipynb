{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "9q0ArgaTU2Wh",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Week 4 : Regularization & Dimensionality Reduction \n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "1. Regularization : Lasso and Ridge\n",
    "2. Selecting optimal alpha for regularization\n",
    "3. Dimensionality Reduction from scratch with numpy\n",
    "4. Dimensionality Reduction with Sklearn\n",
    "5. Assignment 1\n",
    "```\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5iPsyEnU2Wl"
   },
   "source": [
    "## 1. Regularization : Lasso and Ridge\n",
    "\n",
    "Both models are the regularized forms of the linear regression.\n",
    "Lasso with L1 regularization and Ridge with L2 regularization.\n",
    "Both act as a constraint region for the coeffeicients/weight, where they must reside in.\n",
    "\n",
    "### Issues:\n",
    "1. When to use Lasso?\n",
    "2. When to use Ridge?\n",
    "3. Since it is hard to decide the parameters influence, How we can decide which regularization? and decide the value of lambda?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yheCZAN5U2Wm"
   },
   "source": [
    "## 1.1 Loading California housing dataset\n",
    "\n",
    "Housing-Prices Values in Suburbs of California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T17:36:17.901667186Z",
     "start_time": "2023-09-18T17:36:14.329518257Z"
    },
    "id": "_D3cVLqAU2Wm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/8, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FcVZygHU2Wm"
   },
   "source": [
    "## 1.2 Fitting both Lasso and Ridge\n",
    "<span style=\"color:red\">Task:  Fit two models: Lasso and Ridge - with the default alpha.\n",
    "Then print their coefficients and notice the difference.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukdylLxeU2Wn"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "lasso = None\n",
    "lasso.fit(x_train, y_train)\n",
    "ridge = None\n",
    "ridge.fit(x_train, y_train)\n",
    "\n",
    "print('Lasso coef', lasso.coef_)\n",
    "print('Ridge coef', ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OamFphtxU2Wn"
   },
   "outputs": [],
   "source": [
    "lasso.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ydB6mUIIU2Wn",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 1.3 Searching for optimal $\\alpha$\n",
    "\n",
    "<span style=\"color:red\"><strong>Task</strong>: Let's try different values for alpha for Lasso regressor and plot the validation loss.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWqdice2U2Wo"
   },
   "outputs": [],
   "source": [
    "alphas = [2.2, 2, 1.5, 1.3, 1.2, 1.1, 1, 0.3, 0.1]\n",
    "losses = []\n",
    "for alpha in alphas:\n",
    "    # Write (4 lines): create a Lasso regressor with the alpha value.\n",
    "    # Fit it to the training set, then get the prediction of the validation set (x_val).\n",
    "    # calculate the mean squared error loss, then append it to the losses array\n",
    "    pass\n",
    "plt.plot(alphas, losses)\n",
    "plt.title(\"Lasso alpha value selection\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Mean squared error\")\n",
    "plt.show()\n",
    "\n",
    "best_alpha = alphas[np.argmin(losses)]\n",
    "print(\"Best value of alpha:\", best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zWcZWnhVU2Wo",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Measuring the loss on the testset with Lasso regressor with the best alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67XXEIJ7U2Wo"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(best_alpha)\n",
    "lasso.fit(x_train, y_train)\n",
    "y_pred = lasso.predict(x_test)\n",
    "print(\"MSE on testset:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9CdKHWW1U2Wo",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. Principal Component Analysis (PCA)\n",
    "\n",
    "1. How does PCA reduce data dimensionality?\n",
    "2. What is eigenvector?\n",
    "\n",
    "<span style=\"color:red\"><strong>Task:</strong> Now you will implement basic steps of PCA: mean-centering, eigenvectors calculation using covariance matrix, projecting data to the first PC, and restoring it back.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "REhpA2NJU2Wp",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2.1 Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2QwbBKYU2Wp"
   },
   "outputs": [],
   "source": [
    "# N is a sample size\n",
    "N = 25\n",
    "# we can fix a random seed. It allows us to get the same data\n",
    "np.random.seed(10)\n",
    "# form our data\n",
    "x = np.linspace(-5, -3, N)\n",
    "y = 10 + 2*x + np.random.random(size=(N,))\n",
    "data = np.stack([x,y], axis = 1)\n",
    "\n",
    "\n",
    "plt.title(\"Data\")\n",
    "plt.plot(data[:,0], data[:,1], '.', label='Original Data', color=\"green\")\n",
    "plt.legend()\n",
    "\n",
    "plt.axis([-6, 2, -3, 6])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "jBznttPjU2Wp",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.2 Centering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtrZYfCeU2Wp"
   },
   "outputs": [],
   "source": [
    "# center data by subtracting mean value from each feature\n",
    "# pay attention to mean_vector <-- we need it later for restoring our data\n",
    "mean_vector = None\n",
    "data_centered = None\n",
    "\n",
    "plt.title(\"Centered data\")\n",
    "plt.plot(data[:,0], data[:,1], '.', color=\"green\", label=\"Original\")\n",
    "plt.plot(data_centered[:,0], data_centered[:,1], '.', color=\"blue\", label=\"Centered\")\n",
    "plt.axis([-6, 2, -3, 6])\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7ZZMOIxdU2Wq",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.3 Covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWiwhcWJU2Wq"
   },
   "outputs": [],
   "source": [
    "# calculate covariance matrix for our centered data\n",
    "cov_mat = None\n",
    "print('Covariance matrix:\\n', cov_mat)\n",
    "\n",
    "# also, to make sure you understand how to calculate covariance, calculate and print cov(X,Y)\n",
    "# check that it is the same as in the covariance matrix\n",
    "cov_xy = 1/(N-1) * np.sum(data_centered[:,0] * data_centered[:,1])\n",
    "print('\\ncov(X,Y):', cov_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "VSGGAZOIU2Wq",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.4 Eigenvectors and eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMa6ZPD3U2Wq"
   },
   "outputs": [],
   "source": [
    "# compute eigenvectors and eigenvalues, print them\n",
    "eig_values, eig_vectors = None\n",
    "print('Eigenvalues:', eig_values)\n",
    "print('Eigenvectors:\\n', eig_vectors)\n",
    "\n",
    "# are they already in the needed order?\n",
    "# order eigenvectors and eigenvalues by eigenvalues, descending\n",
    "idx = None\n",
    "eig_values = eig_values[idx]\n",
    "eig_vectors = eig_vectors[:,idx]\n",
    "print('\\nsorted eigenvalues:', eig_values)\n",
    "print('sorted Eigenvectors:\\n', eig_vectors)\n",
    "\n",
    "# estimate variance retained by each principal component\n",
    "retained_var = eig_values / eig_values.sum()\n",
    "print('\\nRetained variance:',   retained_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "I0Oy9ZvxU2Wr",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.5 Project data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgBRQPocU2Wr"
   },
   "outputs": [],
   "source": [
    "# project data to the first principal component\n",
    "first_pc = np.expand_dims(eig_vectors[:,0], axis=1)\n",
    "projected_data = None\n",
    "\n",
    "plt.title(\"Projected data\")\n",
    "plt.plot(data[:,0], data[:,1], '.', color=\"green\", label=\"Original\")\n",
    "plt.plot(data_centered[:,0], data_centered[:,1], '.', color=\"blue\", label=\"centered\")\n",
    "plt.plot(projected_data, np.ones(len(projected_data)), '.', color=\"red\",  label=\"projected\")\n",
    "plt.axis([-6, 3, -3, 6])\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1aPIyNuQU2Wr",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.6 Restore data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OWjl7emU2Wr"
   },
   "outputs": [],
   "source": [
    "# project data back to initial space\n",
    "# remember to add a mean_vector to the restored data\n",
    "restored_data = np.dot(projected_data, first_pc.T) + mean_vector\n",
    "\n",
    "plt.title(\"Restored data\")\n",
    "plt.plot(data[:,0], data[:,1], '.', color=\"green\")\n",
    "plt.plot(data_centered[:,0], data_centered[:,1], '.', color=\"blue\")\n",
    "plt.plot(restored_data[:,0], restored_data[:,1], '.', color=\"red\")\n",
    "plt.axis([-6, 2, -3, 6])\n",
    "plt.grid('True')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "65q7NTqLU2Wr",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. SKLEARN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TzO4O73U2Wr"
   },
   "outputs": [],
   "source": [
    "# this is to check your solution\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "x_PCA = pca.fit_transform(data)\n",
    "\n",
    "plt.title(\"Projected data\")\n",
    "plt.plot(data[:,0], data[:,1], '.', color=\"green\", label=\"Original\")\n",
    "plt.plot(data_centered[:,0], data_centered[:,1], '.', color=\"blue\", label=\"Centered\")\n",
    "plt.plot(x_PCA, np.zeros(len(projected_data)), '.', color=\"red\", label=\"Projected\")\n",
    "plt.axis([-6, 3, -3, 6])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid('True')\n",
    "plt.legend()\n",
    "\n",
    "print('Mean : ', pca.mean_)\n",
    "print('explained variance : ',pca.explained_variance_)\n",
    "print('explained variance ratio: ',pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSU-yY1RXfjg"
   },
   "source": [
    "### explained variance ratio\n",
    "\n",
    "It provides insights into the amount of information retained by each principal component and helps assess their significance in capturing the underlying patterns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETxn-DuNU2Ws"
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdZxYrBLU2Ws"
   },
   "source": [
    "## 3. SKLEARN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diHfIEnyU2Ws",
    "outputId": "422eeb71-4163-4c24-ea76-8edb847c29df"
   },
   "outputs": [],
   "source": [
    "# this is to check your solution\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "x_PCA = pca.fit_transform(data)\n",
    "\n",
    "plt.title(\"Projected data\")\n",
    "plt.plot(data[:,0], data[:,1], '.', color=\"green\", label=\"Original\")\n",
    "plt.plot(data_centered[:,0], data_centered[:,1], '.', color=\"blue\", label=\"Centered\")\n",
    "plt.plot(x_PCA, np.zeros(len(projected_data)), '.', color=\"red\", label=\"Projected\")\n",
    "plt.axis([-6, 3, -3, 6])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid('True')\n",
    "plt.legend()\n",
    "\n",
    "print('Mean : ',pca.mean_)\n",
    "print('explained variance : ',pca.explained_variance_)\n",
    "print('explained variance ratio: ',pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFjH7G1tU2Wt"
   },
   "source": [
    "## <center>Self-Practice  Task</center>\n",
    "\n",
    "* Using dataset given for you Assignment task 2.\n",
    "    1. Train a logistic regression model with Regularization\n",
    "    1. Train a logistic regression model with data after PCA\n",
    "    1. Calculate Accuracy, Precision and Recall values for each of the above trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIAFmkjYU2Wu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
